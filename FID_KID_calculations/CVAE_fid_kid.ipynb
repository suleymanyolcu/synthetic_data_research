{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f7c7f4ea-7481-497c-b4a2-3e0e42f2a95f",
   "metadata": {},
   "source": [
    "# Setting up GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b36cdb40-89f1-4210-9b9b-800a8815d616",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
      "env: CUDA_VISIBLE_DEVICES=3\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_DEVICE_ORDER=PCI_BUS_ID\n",
    "%env CUDA_VISIBLE_DEVICES=3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf6d6077-0618-4948-ae08-be5bc54d72a2",
   "metadata": {},
   "source": [
    "# Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "394a1c3b-f27d-46cc-877f-d8878a1f716e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'  \n",
    "os.environ['XLA_FLAGS'] = '--xla_gpu_cuda_data_dir=/dev/null'\n",
    "import cv2\n",
    "import pathlib\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import layers, Model\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.metrics import f1_score, recall_score, precision_score\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from scipy.linalg import sqrtm\n",
    "from sklearn.metrics.pairwise import polynomial_kernel\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", message=\"No artists with labels found to put in legend.\")  \n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "SEED = 64\n",
    "tf.random.set_seed(SEED)\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a597f28-5009-4d02-912f-6b48a742deec",
   "metadata": {},
   "source": [
    "# Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4ff1346-732c-4d71-9fdc-cd2f437c3887",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReadDataset:\n",
    "    def __init__(self, datasetpath, labels, image_shape):\n",
    "        self.datasetpath = datasetpath\n",
    "        self.labels = labels\n",
    "        self.image_shape = image_shape\n",
    "    def returListImages(self,):\n",
    "        self.images = []\n",
    "        for label in self.labels:\n",
    "            self.images.append(list(pathlib.Path(os.path.join(self.datasetpath,\n",
    "                                                              label)).glob('*.*')))\n",
    "    def readImages(self,):\n",
    "        self.returListImages()\n",
    "        self.finalImages = []\n",
    "        labels = []\n",
    "        for label in range(len(self.labels)):\n",
    "            for img in self.images[label]:\n",
    "                img = cv2.imread(str(img), cv2.IMREAD_GRAYSCALE)\n",
    "                if img is None:\n",
    "                    continue\n",
    "                img = cv2.resize(img , self.image_shape[:2])\n",
    "                img  = img/255\n",
    "                img = np.expand_dims(img, axis=-1)  # Add channel dimension to get shape (64, 64, 1)\n",
    "                self.finalImages.append(img)\n",
    "                labels.append(label)\n",
    "        images = np.array(self.finalImages)\n",
    "        labels = np.array(labels)\n",
    "        return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ccfe2b43-9ee1-41b8-a7a1-c1f39dc82e9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((5216, 64, 64, 1), (5216,))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "readDatasetObject = ReadDataset('chest_xray/train',\n",
    "                               ['NORMAL', 'PNEUMONIA'],\n",
    "                               (64, 64))\n",
    "images, labels = readDatasetObject.readImages()\n",
    "images.shape, labels.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "863b72f2-a08a-4ddc-be27-d53ace665214",
   "metadata": {},
   "source": [
    "# Defining the VAE module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "be984e8e-11c6-4249-8b9c-4e3cbc69ed3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CVAE(Model):\n",
    "    def __init__(self, latent_dim, num_classes, input_shape):\n",
    "        super(CVAE, self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        # Encoder network\n",
    "        self.encoder_inputs = layers.Input(shape=(input_shape[0], input_shape[1], 1))\n",
    "        self.label_inputs = layers.Input(shape=(num_classes,))\n",
    "        \n",
    "        x = layers.Conv2D(32, (3, 3), activation='relu', strides=2, padding='same')(self.encoder_inputs)\n",
    "        x = layers.Conv2D(64, (3, 3), activation='relu', strides=2, padding='same')(x)\n",
    "        x = layers.Flatten()(x)\n",
    "        x = layers.Concatenate()([x, self.label_inputs])\n",
    "        x = layers.Dense(128, activation='relu')(x)\n",
    "        \n",
    "        self.z_mean = layers.Dense(latent_dim)(x)\n",
    "        self.z_log_var = layers.Dense(latent_dim)(x)\n",
    "\n",
    "        # Decoder network\n",
    "        self.decoder_inputs = layers.Input(shape=(latent_dim,))\n",
    "        self.decoder_labels = layers.Input(shape=(num_classes,))\n",
    "        x = layers.Concatenate()([self.decoder_inputs, self.decoder_labels])\n",
    "        x = layers.Dense(16 * 16 * 64, activation='relu')(x)\n",
    "        x = layers.Reshape((16, 16, 64))(x)\n",
    "        x = layers.Conv2DTranspose(64, (3, 3), activation='relu', strides=2, padding='same')(x)\n",
    "        x = layers.Conv2DTranspose(32, (3, 3), activation='relu', strides=2, padding='same')(x)\n",
    "        decoder_outputs = layers.Conv2DTranspose(1, (3, 3), activation='sigmoid', padding='same')(x)\n",
    "\n",
    "        self.encoder = Model([self.encoder_inputs, self.label_inputs], [self.z_mean, self.z_log_var])\n",
    "        self.decoder = Model([self.decoder_inputs, self.decoder_labels], decoder_outputs)\n",
    "\n",
    "    def sample(self, eps=None):\n",
    "        if eps is None:\n",
    "            eps = tf.random.normal(shape=(self.latent_dim,))\n",
    "        return self.decode(eps, apply_sigmoid=True)\n",
    "\n",
    "    def encode(self, x, labels):\n",
    "        z_mean, z_log_var = self.encoder([x, labels])\n",
    "        eps = tf.random.normal(shape=z_mean.shape)\n",
    "        z = z_mean + tf.exp(0.5 * z_log_var) * eps\n",
    "        return z, z_mean, z_log_var\n",
    "\n",
    "    def decode(self, z, labels, apply_sigmoid=False):\n",
    "        logits = self.decoder([z, labels])\n",
    "        if apply_sigmoid:\n",
    "            probs = tf.sigmoid(logits)\n",
    "            return probs\n",
    "        return logits\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x, labels = inputs\n",
    "        z, z_mean, z_log_var = self.encode(x, labels)\n",
    "        reconstructed = self.decode(z, labels)\n",
    "        return reconstructed, z_mean, z_log_var\n",
    "\n",
    "# Example usage\n",
    "latent_dim = 16\n",
    "num_classes = 2\n",
    "input_shape = (64, 64)\n",
    "\n",
    "cvae = CVAE(latent_dim, num_classes, input_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "759e44cf-5f2f-4c16-a3e9-dca1a22b01c9",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "648aee50-e64b-4c21-aabc-f6916446043c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1728215781.938922  322772 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 completed\n",
      "Epoch 50 completed\n",
      "Epoch 100 completed\n",
      "Epoch 150 completed\n",
      "Epoch 200 completed\n",
      "Epoch 250 completed\n",
      "Epoch 300 completed\n",
      "Epoch 350 completed\n"
     ]
    }
   ],
   "source": [
    "# Loss function\n",
    "def compute_loss(model, x, labels):\n",
    "    z, z_mean, z_log_var = model.encode(x, labels)\n",
    "    x_reconstructed = model.decode(z, labels)\n",
    "    reconstruction_loss = tf.reduce_mean(tf.reduce_sum(tf.keras.losses.binary_crossentropy(x, x_reconstructed), axis=(1, 2)))\n",
    "    kl_loss = -0.5 * tf.reduce_mean(tf.reduce_sum(1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var), axis=1))\n",
    "    return reconstruction_loss + kl_loss\n",
    "\n",
    "# Training step\n",
    "@tf.function\n",
    "def train_step(model, x, labels, optimizer):\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss = compute_loss(model, x, labels)\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "# Convert labels to one-hot encoding\n",
    "def one_hot_encode_labels(labels, num_classes):\n",
    "    return tf.one_hot(labels, num_classes)\n",
    "\n",
    "# Prepare the dataset\n",
    "images = np.array(images).astype('float32')\n",
    "labels = one_hot_encode_labels(labels, num_classes)\n",
    "\n",
    "# Create a TensorFlow dataset\n",
    "batch_size = 32\n",
    "dataset = tf.data.Dataset.from_tensor_slices((images, labels)).shuffle(10000).batch(batch_size)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = Adam(learning_rate=1e-3)\n",
    "\n",
    "# Training loop\n",
    "epochs = 351\n",
    "for epoch in range(epochs):\n",
    "    for x_batch, labels_batch in dataset:\n",
    "        x_batch = tf.convert_to_tensor(x_batch)\n",
    "        labels_batch = tf.convert_to_tensor(labels_batch)\n",
    "        train_step(cvae, x_batch, labels_batch, optimizer)\n",
    "    if epoch %50 == 0:\n",
    "        print(f'Epoch {epoch} completed')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46cb1c5a-4b49-48cf-a93e-b65b28a61fc2",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d3e513df-8c2f-4cf1-a870-53fba4a2a505",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_fid(real_images, generated_images):\n",
    "    import numpy as np\n",
    "    from scipy.linalg import sqrtm\n",
    "    from keras.applications.inception_v3 import InceptionV3, preprocess_input\n",
    "    import tensorflow as tf\n",
    "\n",
    "    # Resize images to 299x299\n",
    "    real_images_resized = tf.image.resize(real_images, (299, 299), method='bicubic')\n",
    "    generated_images_resized = tf.image.resize(generated_images, (299, 299), method='bicubic')\n",
    "\n",
    "    # Load InceptionV3 model\n",
    "    model = InceptionV3(include_top=False, pooling='avg', input_shape=(299, 299, 3))\n",
    "\n",
    "    # Get activations\n",
    "    act_real = model.predict(real_images_resized)\n",
    "    act_gen = model.predict(generated_images_resized)\n",
    "\n",
    "    # Calculate mean and covariance\n",
    "    mu_real, sigma_real = np.mean(act_real, axis=0), np.cov(act_real, rowvar=False)\n",
    "    mu_gen, sigma_gen = np.mean(act_gen, axis=0), np.cov(act_gen, rowvar=False)\n",
    "\n",
    "    # Calculate FID score\n",
    "    ssdiff = np.sum((mu_real - mu_gen) ** 2)\n",
    "    covmean = sqrtm(sigma_real.dot(sigma_gen))\n",
    "\n",
    "    # Numerical error handling\n",
    "    if np.iscomplexobj(covmean):\n",
    "        covmean = covmean.real\n",
    "\n",
    "    fid = ssdiff + np.trace(sigma_real + sigma_gen - 2 * covmean)\n",
    "    return fid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e8603487-2ac3-4ce0-be47-86a502386eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_kid(real_images, generated_images):\n",
    "    import numpy as np\n",
    "    from scipy.linalg import sqrtm\n",
    "    from sklearn.metrics.pairwise import polynomial_kernel\n",
    "    from keras.applications.inception_v3 import InceptionV3, preprocess_input\n",
    "    import tensorflow as tf\n",
    "\n",
    "    # Resize images to 299x299 pixels\n",
    "    real_images_resized = tf.image.resize(real_images, (299, 299), method='bicubic')\n",
    "    generated_images_resized = tf.image.resize(generated_images, (299, 299), method='bicubic')\n",
    "\n",
    "\n",
    "    # Load pre-trained InceptionV3 model\n",
    "    inception_model = InceptionV3(include_top=False, pooling='avg', input_shape=(299, 299, 3))\n",
    "\n",
    "    # Get the activations\n",
    "    act_real = inception_model.predict(real_images_resized)\n",
    "    act_gen = inception_model.predict(generated_images_resized)\n",
    "\n",
    "    # Compute polynomial kernels\n",
    "    kernel_real = polynomial_kernel(act_real, degree=3, gamma=None, coef0=1)\n",
    "    kernel_gen = polynomial_kernel(act_gen, degree=3, gamma=None, coef0=1)\n",
    "    kernel_cross = polynomial_kernel(act_real, act_gen, degree=3, gamma=None, coef0=1)\n",
    "\n",
    "    m = len(act_real)\n",
    "    n = len(act_gen)\n",
    "\n",
    "    # Calculate KID score\n",
    "    kid = (np.sum(kernel_real) / (m * m)) + (np.sum(kernel_gen) / (n * n)) - (2 * np.sum(kernel_cross) / (m * n))\n",
    "    return kid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3ac1fa86-6ebb-4c46-81a2-bae425f76a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_grayscale_to_rgb(images):\n",
    "    # Convert numpy array to TensorFlow tensor\n",
    "    images_tensor = tf.convert_to_tensor(images)\n",
    "    # Convert grayscale images to RGB\n",
    "    return tf.image.grayscale_to_rgb(images_tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "82f96e0a-7aec-44b4-9c0f-57a4eefd4223",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_tensor(tensor):\n",
    "    # Find the minimum and maximum values of the tensor\n",
    "    min_val = tf.reduce_min(tensor)\n",
    "    max_val = tf.reduce_max(tensor)\n",
    "    \n",
    "    # Normalize the tensor to [0, 1]\n",
    "    normalized_tensor = (tensor - min_val) / (max_val - min_val)\n",
    "    \n",
    "    # Scale the normalized tensor to [-1, 1]\n",
    "    normalized_tensor_neg_one_to_one = (normalized_tensor * 2) - 1\n",
    "    \n",
    "    return normalized_tensor_neg_one_to_one\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "77fe1f25-8987-4afd-95ca-2a3ffdd16259",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "163/163 [==============================] - 9s 38ms/step\n",
      "163/163 [==============================] - 7s 40ms/step\n",
      "163/163 [==============================] - 7s 39ms/step\n",
      "163/163 [==============================] - 8s 46ms/step\n",
      "FID Score: 277.36191202436197\n",
      "KID Score: 0.3541805358218224\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_images(images, num_images=20, image_size=(64, 64), title=\"Generated Images\"):\n",
    "    # Calculate the grid size\n",
    "    grid_size = int(np.ceil(np.sqrt(num_images)))\n",
    "    \n",
    "    # Create the plot\n",
    "    plt.figure(figsize=(grid_size * 2, grid_size * 2))\n",
    "    \n",
    "    for i in range(num_images):\n",
    "        ax = plt.subplot(grid_size, grid_size, i + 1)\n",
    "        plt.imshow(images[i], cmap='gray')\n",
    "        plt.axis('off')\n",
    "    \n",
    "    if title:\n",
    "        plt.suptitle(title, fontsize=16)\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "# Generate synthetic images on CPU\n",
    "with tf.device('/CPU:0'):  # Ensure operations run on the CPU\n",
    "    latent_vectors = np.random.normal(size=(5216, latent_dim))  # Generate random latent vectors\n",
    "    newlabels = tf.keras.utils.to_categorical(np.random.choice([0, 1], size=(5216,)), num_classes=2)\n",
    "\n",
    "    # Generate synthetic images using the decoder from the CVAE model on CPU\n",
    "    generated_images = []\n",
    "    batch_size = 1000  # Adjust the batch size to prevent memory exhaustion\n",
    "    for i in range(0, 5216, batch_size):\n",
    "        latent_batch = latent_vectors[i:i+batch_size]\n",
    "        label_batch = newlabels[i:i+batch_size]\n",
    "        generated_images_batch = cvae.decode(latent_batch, label_batch, apply_sigmoid=True)\n",
    "        generated_images.append(generated_images_batch)\n",
    "    \n",
    "    generated_images = np.concatenate(generated_images, axis=0)\n",
    "\n",
    "normalized_images = normalize_tensor(images)\n",
    "normalized_generated_images = normalize_tensor(generated_images)\n",
    "\n",
    "# Convert grayscale images to RGB\n",
    "real_images_rgb = convert_grayscale_to_rgb(images)\n",
    "generated_images_rgb = convert_grayscale_to_rgb(generated_images)\n",
    "\n",
    "# Calculate FID and KID scores\n",
    "fid_score = calculate_fid(real_images_rgb, generated_images_rgb)\n",
    "kid_score = calculate_kid(real_images_rgb, generated_images_rgb)\n",
    "\n",
    "print(f'FID Score: {fid_score}')\n",
    "print(f'KID Score: {kid_score}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
